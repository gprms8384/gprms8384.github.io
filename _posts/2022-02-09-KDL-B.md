---
title : "그래디언트 기반 최적화"
excerpt : "변화율과 확률적경사하강법, 역전파 알고리즘"

categories:
    - KDL
tages:
    - KDL

toc : true
toc_sticky: true

date: 2022-02-09
#last_modified_at: 2021-09-11
---
신경망에서의 각 층은 입력 데이터를 변환한다.
- 각 층의 속성이 존재하는데 이 속성을 **가중치** 또는 **훈련되는 파라미터** 라고 한다. 가중치에는 훈련 데이터를 신경망에 노출시켜 학습된 정보가 담겨 있다.
- 가중치는 피드백에 기초해 조정된다. 이 가중치 조정은 머신러닝의 핵심이라고도 볼 수 있다.
# 1. 훈련 반복 루프
1. 훈련 샘플 x와 이에 상응하는 타깃 y를 추출한다.
2. x를 사용하여 네트워크를 실행하고, 예측 y_pred를 구한다.
3. y_pred와 y의 차이를 측정하여 배치에 대한 네트워크 손실을 계산한다.
4. 배치에 대한 손실이 조금씩 감소될 수 있도록 네트워크의 모든 가중치를 업데이트 한다

- 다양한 가중치를 수정하는 방법은 어려운 방법이다. 이를위한 가장 간단한 방법은 네트워크 가중치 행렬의 원소를 모두 고정하고 관심있는 하나만 다른 값은 적용해보는 것이다.
- 하지만 이런 방법은 모든 가중치 행렬의 원소마다 두 번읮 ㅓㅇ방향 패스를 계산해야 하므로 비효율적이다.(가중치는 수천 수백개 이상을 가지기 떄문)

# 2. 그래디언트
- 신경망에 사용된 모든 연산이 미분이 가능하다는 장점을 사용해 네트워크 가중치에 대한 손실의 **그래디언트**를 계산하는 것이 더 좋은 방법이다.
- 그래디언트의 반대 방향으로 가중치를 이동하게 되면 손실이 감소한다.
## 1. 변화율?
- 실수 x를 새로운 실수인 y에 연결하는 연속적이고 매끄러운 함수인 f(x) = y를 가정해보면 이 함수는 연속적이기 때문에 x를 바꾸면 y또한 변경된다. 이를 연속성이라고 한다.
- 또한 함수는 매끈하기 떄문에 증가값이 충분히 작다면 어떤 특정 포인트P에서 기울기 a의 선형함수로 f를 근사할 수 있다. 
- 이러한 기울기를 p에서의 f의 변화율이라고 한다. a의 절댓값(변화율의 크기)은 증가나 감소가 얼마나 빠르게 일어나는지 알려준다.
- 미분이 가능하다는 것은 즉 변화율을 유도할 수 있다는 의미로 볼 수 있다. 
- 함수 f(x)에 대해 x의 값을 f의 국부적인 선혀 ㅇ근사인 지점의 기울기로 매핑하는 변화율 함수가 존재한다.
- 변화율 함수는 x가 바뀜에 따라 f(x)가 어떻게 바뀌는지 설명한다.
## 2. 텐서 연산의 변화율 = 그래디언트
- 그래디언트는 텐서 연산의 변화율을 말한다. 
- 다차원입력으로 받는 함수에 변화율 개념을 확장한 것이라 할 수 있다.
# 3. 확률적 경사하강법
- 미분 가능한 함수가 주어지게되면 이롡거으로 최솟값을 해석적으로 구할 수 있다. 이를 신경망에 적용하면 가장 작은 손실 함수의 값을 만드는 가중치의 조합을 해석적으로 찾는 것을 의미한다.(여기서 최솟값은 변화율이 0인 지점)
- gradient(f)(W) = 0의 식을 풀어야한다. 이 식은 N개의 변수로 이루어진 다항식이며, 여기서 N은 가중치의 개수를 말한다.(실제에서는 수천 수만개의 가중치가 존재함)
- 이를 실제로 적용이 불가하기에 랜덤한 배치 데이터에서 현재 손실 값을 토대로 조금씩 파라미터를 수정한다.
- 이를 **미니 배치 확률적 경사하강법**이라고 한다.
- 모든 데이터를 사용하는 배치 SGD(확률적 경사 하강법) 반복마다 하나의 샘플과 하나의타깃만을 뽑는 SGD이 두가의 극단적 방법의 절충안은 위에서 언급한 '미니 배치'를 활용하는 것이다.
- 실제 저차원공간에서의 표현은 고차원 공간에서의 표현과 다르며 이는 오랫동안 딥러닝 분야에서 많은 이슈가 되었다, 신경망 알고리즘에서 지역 최솟값에 쉽게 갇힐 것으로 생각했지만 고차원 공간에서는 대부분 안정점으로 나타나며 지역 최솟값으로 나오는 경우는 드물다.
- 업데이트할 가중치를 계산할 때 현재 그래디언트 값만을 보지 않고 이전에 업데이트된 가중치를 여러 가지 다른 방식으로 고려하는 경우가 있다,(모멘텀을 사용한 SGD, RMSProp, Adagrad 등) 이들을 **옵티마이저**라고 한다.
- 모멘턴 : SGD에서 나타나는 수렴속도와 지역 최솟값을 해결한다. 다음 업데이트를 현재의 기울기 값과(현재가속도) 이전의 가속도로 인한 현재 속도를 함꼐 고려한다.
# 4. 역전파 알고리즘
- 미적분에서 연결된 함수들은 연쇄 법칙 에 유도될 수 있다. 이 연쇄 법칙을 신경망의 그래디언트 계산에 적용한 것을 **역전파 알고리즘**이라 한다.
- 역전파는 최종 손실 값에서부터 시작한다.