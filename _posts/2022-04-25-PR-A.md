---

title : "[논문 리뷰] Attention is all you need - 1"
excerpt : "Transformer 그 서막..."

categories:
    - PR
tages:
    - PR

toc : true
toc_sticky: true

date: 2022-04-25
last_modified_at: 2022-04-25

---
# [논문 리뷰] Attention is all you need - 1

BERT와 GPT 등의 Attention기반의 기초가 되는 Transformer를 제시하 Attention is all you need를 읽어보았다.

일부 번역과 이해한 내용을 적어보고자 한다.

- 초심자의 논문 리뷰이므로 부정확한 부분이 매우 매우 많이 존재할 수 있습니다.(**이해를 잘못한 부분이 있거나 수정을 요하는 부분이 있다면 댓글 부탁드립니다!!!**)

## Abstract



지배적으로 사용되던 sequence transduction 모델들은 encoder와 decoder에 포함한 RNN와 CNN을 기반으로 했으며, 가장 성능이 좋은 모델 또한  Attention mechanism통해  encoder와 decoder를 연결했다. 하지만 이 논문에서는 recurrence와 convolutiosd을 배제하고 오직 attention mechanism에 기반한 'Transformer'를 제안했다.

이는 이전의 모델들보다 좋은 성능을 내주었다.



## Introduction

RNN, LSTM 그리고 GRU 등은 언어 모델링 및 기계번역 등의 시퀀스 모델링 문제에서 많이 사용된다. 하지만 이런 모델들은 필연적으로 이전 결과를 입력으로 받는 순차적 특성으로 인해 병렬처리를 배제한다. 많은 지속적인 개선노력에 의해 factorization trick과 conditional computation을 이용하여 계산 효율이 향상되었다. 하지만 여전히 순차적 계산에 의한 제약은 남아있다.



(논문개제일 기준)Attention mechanism은 다양한 작업에서 sequence model이나 transduction 모델들과 결합되어 사용된다. 

이러한 작업들에서 recurrence모델의 제약을 피하며, 입출력 사이의 전역적 의존성을 이끌어내기위한 ''Transformer'를 제안했다.



## Model architecture

<img width="451" alt="Screen Shot 2022-04-25 at 11 35 08 PM" src="https://user-images.githubusercontent.com/37393115/165111522-ec9f7e42-4727-411c-af9c-46729d11b143.png">

- Transformer 의 모델 구조

### Encoder

인코더는 6개의 동일한 층으로 구성된다.(N=6) 각 층은 두개의 서브 레이어를 가진다. 첫번째는 Multi-head self attention mechanism을 그리고 두번째는 단순한 완전연결 피드포워드 네트워크를 가진다.

2개의 서브레이어에 각각 residual connection을 채택하며, 층 normalization을 시행한다. 또한 각 서브 레이어의 출력은 서브레이어 x에 대한 LayerNorm 스스로 수행한다. 모델의 모든 서브 레이어는 residual connection이 용이하게 하기 위해 512차원으로 임베딩을 수행한다.



### Decoder

디코더 또한 6개의 동일한 층으로 구성되어 있다. 추가로 residual connection을 수행한 후 층 normalization을 동일하게 수행한다.

디코더 각 층은 2개의 서브레이어 이외에도 인코더스택의 multi-head attention을 수행한 3번째의 서브 레이어를 삽입한다.

디코더의 self-attention 서브레이어 에서는 현재 위치보다 뒤에 있는 요소에 attention을 적용하지 못하도록 masking을 추가한다. 이를 통해 position i에 대한 예측을 위해 i이전에 있는 정보를 사용하도록 하는 것이다.



### Attention

attention function은 query와 key-value의 쌍을 output에 mapping 하는것으로 설명이 가능하다. 여기서 query, value, key, output은 벡터로 이루어져 있다. 출력은 가중합(weight sum)으로 계산되는데 weight은 해당 key와 query의 compatibility function으로 계산된다.



<img width="600" alt="Screen Shot 2022-04-25 at 11 35 22 PM" src="https://user-images.githubusercontent.com/37393115/165111640-a205bcef-d848-447a-b370-27b3bd993bcf.png">

### Scaled Dot-Product Attention

input은 Dk차원의 쿼리와 키값 그리고 Dv차원의 value 벡터들로 구성된다. 모든 쿼리와 키를 내적(dot product)하고 각각을 루트를 씌운 Dk로 나눠준다, 이후 value들에 대한 가중치를 얻기 위해 softmax 함수를 적용한다.

각각의 Q K V는 학습이 가능한 벡터이다. 따라서 이들을 지속적으로 학습한다면 중요도를 나타낼 수 있게 된다.  어떠한 문장에서 Q값과 K값을 곱하여 softmax를 취해주면 확률값이 나오게 되는데, 이를 통해 어떤 값이 가중치가 높은지 수치로써 알 수 있다. 다음단어를 예측하는데 어떤 단어가 큰 영향을 미치는지 알 수 있게 되는 것이다. 이를 다시 value에 곱해주면 이해할 단어의 attention값이 되는 것이다.

수식은 아래와 같다. (normalize term인  루트dk분의 1은 attention값이 극단적으로 가는 것을 방지해준다.)

<img width="334" alt="Screen Shot 2022-04-25 at 11 35 35 PM" src="https://user-images.githubusercontent.com/37393115/165111738-1a42c25f-7944-4a4c-a0a4-20c6395bbcc1.png">



### Multi-Head Attention

단일의 attention을 두는 것보다 multi head attention을 사용하는것이 효율적이다. 즉, 병렬로 어려번 나누어 사용하는것이 더 효과적이라는 것이다. 각각의 linear projection을 진행한뒤 이를 병렬적으로 수행하고 각각의 projection에서 서로다른 h개의 Dv차원의 결과를 얻고 이를 다 더하고 한번더 linear projection을 수행해 최종 결과를 얻는다.

<img width="445" alt="Screen Shot 2022-04-25 at 11 35 47 PM" src="https://user-images.githubusercontent.com/37393115/165111828-4ff1ddf5-96e7-4bfe-a27f-9f3fe883a2bd.png">
