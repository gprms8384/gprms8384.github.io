---
title : "[혼자공부하는 머신러닝 + 딥러닝] 19_합성곱 신경망"
excerpt : "08-1. 합성곱 신경망"

categories:
    - PML
tages:
    - PML

toc : true
toc_sticky: true

date: 2021-10-11
# last_modified_at: 2021-10-03
---
# 18. 합성곱 신경망의 구성 요소

## 1. 합성곱

- **합성곱**은 입력데이터에 유용한 특성만 드러나게 하는 것으로 볼 수 있다.
    - 합성곱은 입력 데이터 전체에 가중치를 적용하는 것이 아니라 일부에 가중치를 곱한다.
    - 합성곱 층의 뉴런의 가중치는 임의로 정할 수 있는 하이퍼파라미터이다.
    - 이전에 그린 신경망 층은 뉴런이 길게 늘어서 있고 서로 조밀하게 연결되어 있다. 그러나 합성곱에서는 뉴런이 입력 위를 이동하면서 출력을 만들기 때문에 이런 형태를 **합성곱 신경망(CNN)** 이라 부르며 완전연결 신경망과 달리 뉴런을 **필터**라고 부른다.
    - 이 책에서는 케라스 API와 이름을 맞추어 뉴런 개수를 이야기할 떄는 필터라 부르고, 입력에 곱해지는 가중치를 의미할 떄는 커널이라 부르도록 한다.
    - 합성곱은 1차원이 아니라 2차원 입력에도 적용할 수 있다. 물론 입력이 2차원배열이라면 필터도 2차원이여야 한다. - 합성곱 계산을 통해 얻은 출력은 **특성 맵**이라 부른다.
    - 밀집층에서 여러 개의 뉴런을 사용하듯이 합성곱 층에서도 여러 개의 필터를 사용한다. 여러 개의 필터를 사용하면 특성 맵은 순서대로 쌓인다. 
    - 계산이 밀집층과 동일하게 단순히 입력과 가중치를 곱하는 행위를 반복하는 것이다. 2차원 형태를 유지한다는 점이 다르다. 또한 입력보다 훨씬 작은 크기의 커널을 사용하고 입력 위를 이동하면서 2차원 특성 맵을 만든다.
    - 합성곱은 이미지 처리 분야에서 뛰어난 성능을 발휘한다.
    ![그림1](https://user-images.githubusercontent.com/37393115/136744334-9ff15fc4-7e77-4707-afcc-1815825544bb.jpg)


- 케라스의 층은 모두 keras.layers 패키지 아래 클래스로 구현되어있다. 합성곱 층도 마찬가지이다. 
- 합성곱은 Conv2D클래스로 제공한다. 아래 그 예시이다.
from tensorflow import keras
keras.layers.Conv2D(10, kernel_size=(3, 3), activation='relu')
- Conv2D 클래스의 첫 번째 매개변수는 필터의 개수, kernel_size는 필터에 사용할 커널의 크기를 지정한다. 이 두가지는 필수적으로 지정해 주어야 한다. 마지막으로 밀집층에서처럼 활성화 함수를 지정한다.
- 합성곱 신경망은 일반적으로 1개 이상의 합성곱 층을 쓴 인공 신경망을 말한다. 

## 2. 패딩과 스트라이드

- (4, 4)의 입력에 (3, 3)크기의 커널을 적용해 (2, 2)크기의 특성 맵을 만들었다. 그런데 만약 (3, 3)으로 커널을 그대로 두고 출력의 크기를 입력과 동일하게 (4, 4)로 만들수 있을까? 이를 위해서는 마치 더 큰 입력에 합성곱을 수행하는 것처럼 해야한다. 마치(6, 6)의 입력을 가지는 것처럼 말이다. 이렇게 배열 주위에 가상 원소로 채우는 것은 **패딩**이라고 한다. 실제 값이 아니므로 패딩은 0 으로 채운다. 
- 패딩은 실제 값이 0으로만 채워져 있으므로 계산에 영향을 미치는 요소가 없다. 단지 커널의 수행횟수만 증가시켜준다.
- 입력과 특성 맵 크기를 동일하게 만들기 위해 입력 주위에 0으로 패딩하는 것을 **새임 패딩**이라 한다. 합성곱 신경망에서는 새임 패딩이 많이 사용된다. 
- 패딩 없이 순수한 입력 배열에서만 합성곱을 하여 특성 맵을 만드는 경우는 **밸리드 패딩**이라고 한다. 밸리드 패딩은 특성 맵의 크기가 줄어들 수 밖에 없다.
- 패딩은 이미지의 주변에 있는 정보를 잃어버리지 않도록 도와준다. 케라스의 Conv2D 클래스에서는 padding 매개변수로 패딩을 지정할 수 있다. 기본값은 'Valid'로 밸리드 패딩을 나타낸다. 새임패딩을 사용하기 위해서는 'same'으로 지정한다.
![그림2](https://user-images.githubusercontent.com/37393115/136747041-372c8649-df1f-4eee-a132-5e6bc1f75b58.png)

- 지금까지 합성곱 연산은 좌우, 위아래 한 칸씩 이동했다. 하지만 두 칸씩 건너뛸 수도 있다. 이런 이동의 크기를 **스트라이드**라고 한다. 스트라이드의 기본값은 1이다. 이 값은 케라스 Conv2D의 strides 매개변수의 기본값이다.
- strides 매개변수는 오른쪽으로 이동하는 크기와 아래쪽으로 이동하는 크기를 각각 지정도 가능하다.
- 하지만 실제로 둘을 다르게 하거나 스트라이드를 1보다 크게 사용하는 경우도 드물다.
![그림3](https://user-images.githubusercontent.com/37393115/136747045-df6dce49-0851-4265-b787-2a2df24901f3.png)

## 3. 풀링

- **풀링**은 합성곱 층에서 만든 특성 맵의 가로세로 크기를 줄이는 역할을 수행한다. 하지만 실제로 특성 맵의 개수를 줄이지는 않는다. 
- 예를 들자면 (2, 2, 3) 크기의 특성 맵에 풀링을 적용하면 마지막 차원인 개수는 그대로 유지하고 너비와 높이만 줄어들어 (1, 1, 3)크기의 특성 맵이 된다.
- 풀링에는 가중치가 없다. 도장을 찍은 영역에서 가장 큰 값을 고르거나 평균을 계산하는데 이를 각각 **최대 풀링**과 **평균 풀링**이라 부른다.
- 풀링은 가중치가 존재하지 않고, 풀링 크기와 스트라이드가 같다. 그리고 패딩도 없다. 케라스에서는 MaxPooling2D 클래스로 풀링을 수행할 수 있다.
- MaxPooling2D의 첫 번째 매개변수로 풀링의 크기를 지정한다. 합성곱 층과 마찬가지로 strides와 padding 매개변수를 제공한다. strides는 풀링크기와 같고, 패딩도 수행하지 않기 떄문에 'valid'로 되어있다.
- 평균 풀링은 AveragePooling2D이다. 최댓값 대신 평균을 계산하는 것만 빼면 이전과 동일하다. 
- 많은 경우 평균 풀링 모다 최대 풀링을 사용한다. 평균 풀링은 특성 맵에 있는 중요한 정보를 희석시킬 수 있기 때문이다.

## 4. 합성곱 신경망의 전체 구조

![그림4](https://user-images.githubusercontent.com/37393115/136749902-622bd913-fa57-463a-83bd-28c0bffcba31.png)

- 지금까지 합성곱 층, 필터, 패딩, 스트라이드, 풀링 등 중요한 합성곱 신경망의 개념을 모두 살펴보았다.
- 전체적인 구조는 위의 그림에 나타나 있다.
- 각 층에서 주의해야 할 점이나 참고사항은 아래와 같다.
- 따로 언급이 되지 않는다면 합성곱 스트라이드는 항상 1이며, 만들어지는 특성 맵의 크기는 입력과 동일한 크기가 된다. 
- 합성곱 층에도 활성화 함수를 적용하는데 주로 렐루 함수를 많이 사용한다.
- 풀링을 사용하는 이유는 합성곱에서 스트라이드를 크게 하여 특성 맵을 줄이는 것보다 풀링 층에서 크기를 줄이는 것이 경험적으로 더 나은 성능을 내기 때문이다.
- 합성곱 신경망은 합성곱 층에서 특성 맵을 생성하고 풀링에서 크기를 줄이는 구조가 쌍을 이룬다.

## 5. 마치며

- 이번 시간에는 합성곱 신경망에 대해 이해하고, 패딩, 스트라이드, 풀링의 전반적인 개념을 살펴보며 전체 구조를 살펴보았다.
- 다음시간부터 이를 이용해서 실제 이미지 분류작업을 수행해볼 것이다. 신경망을 만들어 가는것이 재미있고, 새로운것을 알아갈 때마다 알게 모르게 성취감을 느낄 수 있어서 배우는것이 즐겁다. 더 열심히 해보려 한다. 
