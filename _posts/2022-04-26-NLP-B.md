---

title : "[NLP]BoW, tf-idf, Stop-Word, N-gram"
excerpt : "개념을 위주로"

categories:
    - NLP
tages:
    - NLP

toc : true
toc_sticky: true

date: 2022-04-26
#last_modified_at: 2021-09-11

---
# [NLP]BoW, tf-idf, Stop-Word, N-gram



## 1. BoW

- BoW(Bage of Word) : 단어들의 순서는 전혀 고려하지 않고 단어들의 출현 빈도에만 집중하는 텍스트 데이터의 수치화 표현 방법이다.

![image](https://user-images.githubusercontent.com/37393115/165201647-7266894c-58da-4459-b4ec-61d321e5d903.png) 



- 단어를 순서를 가지지 않고 전부 집어넣은다음 이를 섞은 다음 어떠한 단어가 N번 등장한다면 N개 존재한다는 것이다,

- 단어를 섞었기 때문에 단어 자체의 순서는 더이상 중요하지 않다. 

- 프로세스느 는 크게 두 과정으로 볼 수 있다.

  1. 각 단어에 고유한 정수 인덱스를 부여한다.
  2. 각 인덱스의 위치에 단어 토큰의 등장 횟수를 기록한 벡터를 만든다.

  ![image](https://user-images.githubusercontent.com/37393115/165202454-94d13c7d-6d2a-4bbd-b13c-4017789b64b7.png)

​	위의 예시에서

​	1. Review 1: This movie is very scary and long

​	2. Review 2: This movie is not scary and is slow

​	3. Review 3: This movie is spooky and good

- 3개의 문장에 등장하는 단어에 인덱스를 부여한 뒤 단어 토큰의 등장 횟수를 기록했다.  이처럼 단어들의 순서와 관계없이 각 단어의 등장 빈도만을 이용해 갯수를 정의한다.



## 2. TF-IDF

- TF-IDF(Term Frequency-Inverse Document Frequency)는 단어의 빈도와 역문서 빈도를 사용해 DTM내의 각 단어들 마다 중요한 정도를 가중치로 주는 방법을 말한다. 
- 먼저 DTM(Document Term Matrix)은 다수의 문서에 등장하는 각 단어들의 빈도를 행렬로 표현한 형태를 말한다.

![image](https://user-images.githubusercontent.com/37393115/165203057-748ed90a-5466-488a-aa2d-1838d67af8c0.png)

- TF-IDF는 TF(현재 문서또는 문장에서의 단어 빈도수)와 IDF(역문서 빈도(단어가 나오는 문서의 총개수의 역))의 곱을 나타낸다. 

- TF-IDF를 공식으로 정의하면 아래와 같다.

  ![image](https://user-images.githubusercontent.com/37393115/165203510-a42184c8-3c40-466c-a5a6-a6a8737792dc.png)

- 공식을 살펴보게 되면 기존의 BoW에서 문서내의 단어 빈도수를 고려하면서, 동시에 다른 문서에도 자주 등장하는 단어는 중요도를 낮추어 점수를 계산한다. 

- **BoW와 TF-IDF Vector의 장단점**

  장점

  - 간단한 구조를 보이지만 이를 통해 주제 분류나 문서 검색 등의 task에서는 좋은 성능을 보인다.

  단점

  - 순서가 중요한 문제에서는 사용하기 힘들다.(기계 번역 등)
  - 단어가 길어지거나 커질수록 사용하기 힘들다.(단어가 길어지면 벡터의길이도늘어나게 된다.)
  - 순서가 존재하지 않으니 단어간의 관계를 표현하지 못한다.

## 3. Stop-Word

- 가지고 있는 데이터에서 유의미한 단어 토큰만을 선별하기 위해서 큰 의미가 없는 단어 토큰을 제거하는 작업이 필요하다.
- 여기서 큰 의미를 가지지 못하는 단어를 '불용어(Stop-Word)'라고 정의한다. 영어 문장에서의 I, My, Me, over등 조사, 접미사 등의 단어들은 문장에 자주 등장하지만 실제 의미 분석에서 차지하는 비중이 작다. 이를 불용어로 처리할 수 있다.

## 4. N-gram

- 이전에 살펴본 BoW, TF-IDF와 함께 빈도를 사용하여 벡터로 단어를 분해하는 한 방법이다. 이전의 방식들은 한 개의 단어만을 대상으로 삼았지만 N-gram은 한 단어 이상의 단어 시퀀스를 분석 대상으로 한다. N-gram에서 N에 따라 단어 시퀀스를 몇개의 단어로 구성할 지를 결정한다. 
- N-gram은 너무 크지 않은 선에서 사용해야 한다.



여기까지 BoW, TF-IDF, Stop-Word, N-gram에 대해 알아보았습니다.

출처 : <https://yngie-c.github.io/nlp/2020/05/22/nlp_ngram/>
      <https://wikidocs.net/22530>
      <https://en.wikipedia.org/wiki/N-gram>
      <https://jiho-ml.com/weekly-nlp-2/>
      <https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=horajjan&logNo=221429936248>