---

title : "순환 신경망과 LSTM, GRU 그리고 Seq2Seq, Attention Mechanism"
excerpt : "RNN, LSTM, GRU, Seq2Seq, Attention"

categories:
    - NLP
tages:
    - NLP

toc : true
toc_sticky: true

date: 2022-04-11
#last_modified_at: 2021-09-11

---

## 순환 신경망과 Seq2Seq 그리고 어텐션 매커니즘

### 순환 신경망(Recurrent Neural Network, RNN)

- 입력과 출력을 시퀀스 단위로 처리하는 시퀀스 모델을 말한다.

- 은닉층의 노드에서 활성화 함수를 통해 나온 결과값을 출력층 방향으로도 보내면서 다시 은닉층 노드의 다음 계산의 입력으로 보낸다.

<img src="https://wikidocs.net/images/page/22886/rnn_image1_ver2.PNG" title="" alt="" data-align="center">

- 은닉층에서 활성화 함수를 통해 결과를 내보내는 역할을 하는 노드는 셀이라고 하며 이 셀은 이전의 값을 기억하는 일종의 메모리 역할을 하여 **메모리 셀**또는 **RNN셀**이라고 한다.

- RNN에서 현재시점을 t라고 한다면 현재의 t는 이전 시점의 메모리셀들의 영향을 받은 값으로 볼 수 있다.  이처럼 메모리 셀이 출력층 방향 또는 다음 시점인 t+1의 자신에게 보내는 값을 은닉상태라고 한다. 현재의 t는 t-1시점의 메모리셀이 보낸 은닉상태값을 t시점의 은닉상태 계산을 위한 입력값으로 사용하는 것이다.

- RNN은 입력과 출력의 길이를 다르게 설계할 수 있다. 그러므로 다양한 용도로 사용할 수 있다. (이미지 캡셔닝, 번역, 텍스트 분류 등)

<img src="https://wikidocs.net/images/page/22886/rnn_image3_ver2.PNG" title="" alt="" data-align="center">

### 장단기 메모리(Long Short-Term Memory, LSTM)

- 가장 단순한 형태의 RNN의 한계
  
  - 바닐라RNN(가장 기본적 형태의 RNN)은 이전 계산 결과에 의존한다. 이는 시점이 길어질 수록 앞의 정보가 뒤로 충분히 전달되지 못하는 현상이 나타난다. 이는 만약 가장 중요한 정보가 맨 앞에 존재할 때 이를 캐치하지 못하는 문제로 이어질 수 있다. 이를 **장기 의존성 문제**라고 한다.
    
    <img src="https://wikidocs.net/images/page/22888/vanilla_rnn_ver2.PNG" title="" alt="" data-align="center">
  
  - 바닐라RNN의 원리를 파헤쳐보면 위와 같다. xt와 ht-1이라는 두개의 입력이 가중치와 곱해져 메모리셀의 입력이 된다.이를 하이퍼볼릭탄젠트 함수의 입력으로 사용하고 이 값은 은닉층의 출력인 은닉상태가 된다.
  
  <img src="https://wikidocs.net/images/page/22888/vaniila_rnn_and_different_lstm_ver2.PNG" title="" alt="" data-align="center">
  
  - 반면 LSTM은 은닉층의 메모리 셀에 입력 게이트, 망각게이트, 출력 게이트를 추가해 불필요한 기억을 지우고, 기억해야할 것들을 정한다. 기존과 다르게 t시점의 상태를 나타내는 셀 상태를 C로 정의한다. 기존의 전통적 RNN보다 긴 시퀀스의 입력을 처리하는데 탁월한 성능을 보인다.

### 게이트 순환 유닛(Gated Recurrent Unit, GRU)

- GRU는 LSTM의 장기 의존성 문제에 대한 해결책을 유지하면서, 은닉 상태를 업데이트하는 계산을 줄여 LSTM보다 간단화 된 구조를 가진다.

- LSTM에서는 출력, 입력, 삭제 게이트라는 3개의 게이트가 존재했다. 반면, GRU에서는 업데이트 게이트와 리셋 게이트 두가지 게이트만 존재한다. 

<img src="https://wikidocs.net/images/page/22889/gru.PNG" title="" alt="" data-align="center">

GRU와 LSTM 중 어떤 것이 모델의 성능면에서 더 낫다라고 단정지어 말할 수 없으며, 기존에 LSTM을 사용하면서 최적의 하이퍼파라미터를 찾아낸 상황이라면 굳이 GRU로 바꿔서 사용할 필요가 없다.



### Seq2Seq

- 시퀀스 투 시퀀스로 입력된 시퀀스로부터 다른 도메인의 시퀀스를 출력하는 형태의 모델이다.

<img src="https://wikidocs.net/images/page/24996/seq2seq%EB%AA%A8%EB%8D%B811.PNG" title="" alt="" data-align="center">

- seq2seq는 인코더와 디코더 두개의 모듈로 구성된다. 인코더는 입력 문장의 정보를 압축해 컨텍스트 벡터를 만들고 이를 디코더에 전송하고, 디코더는 순차적으로 출력한다.

- 인코더는 RNN을 이어놓은 형태를 가진다. 이를 이용해 데이터를 Hidden State Vector(h)로 변환한다. 이 벡터 h는 마지막 RNN셀의 Hidden State이므로 이는 고정길이 벡터이다.

- 즉, 인코더의 역할을 임의 길이의 데이터를 고정 길이의 벡터로 변환하는 작업을 말한다.

- 디코더는 인코더로 부터 h를 넘겨받는다. 그리고 이와 첫번째 문장의 심볼의 입력을 바탕으로 새로운 h(Hidden State)를 계산하고 이를 다른 계층을 건너 예측을 실행한다.

### BLEU(Bilingual Evaluation Understudy)

- 기계 번역 결과와 사람이 직접 번역한 결과가 얼마나 유사한지 비교하여 번역에 대한 성능을 측정하는 방법(측정 기준 : n-gram)

### Seq2Seq 문제점

- 하나의 고정된 크기의 벡터에 모든 정보를 압축하려하여 정보 손실이 발생

- RNN의 고질적 문제인 기울기 소실의 문제가 발생

- **기울기 소실?**
  
  - 학습이 진행되면서 각 파라미터에 대한 가중치의 미분값(경사)가 매우 작아져 0에 가깝게 되는 현상이다.신경망학습 시간이 오래 걸리거나 학습 진행 중에 수렴하는 결과를 야기한다.

- 이를 해결하기 위해 Attention 매커니즘을 도입했다.

### Attention?

- 영어로 주목하다의 뜻을 가진다. 즉 한가지를 집중해서 본다는 의미라고 볼 수 있다. 이는 주요 자연어처리에 사용된다. attention은 해당 시점에서 예측해야 하는 단어와 관련있는 입력을 주요하게 본다고 이야기할 수 있다.
- 디코더에서 출력 단어를 예측하는 시점마다 인코더에서의 전체 입력문장을 한번 참고하는 것(예측해야 하는 단어와 연관이 있는 입력 단어 부분을 집중(attention)해서 보게 된다.
- 어텐션은 처음음 seq2seq에서 성능을 보정하기 위한 목적으로 소개되었으나 현재는 어텐션 스스로가 seq2seq를 대체하는 방법이 되고 있다.



### 어텐션 함수

<img src="https://wikidocs.net/images/page/22893/%EC%BF%BC%EB%A6%AC.PNG" title="" alt="" data-align="center">

Attention(Q, K, V) = Attention Value

- 주어진 쿼리에 대해 키와의 유사도를 구하고 이를 맵핑된 각각의 값에 반영한다. 그리고 유사도가 맵핑된 값을 모두 더해 리턴한다.

### 어텐션 스코어 구하기

<img src="https://wikidocs.net/images/page/22893/dotproductattention2_final.PNG" title="" alt="" data-align="center">

- 시점을 1~N까지라고 할때 h는 인코더의 은닉상태, 디코더의 은닉상태를 s라고 하며, 둘의 차원을 같다고 할때 어텐션 매커니즘에서 단어 예측을 위해서는 t번째 단어를 예측하기 위한 어텐션 값을 설정한다. 이를 a라고 정의한다.
  
  a는 어텐션 스코어라고한다. 어텐션 스코어는 디코더의 시점 t에서 단어를 예측하기 위해 인코더의 모든 은닉 상태 각각이 디코더의 현재 시점의 은닉상태인 s와 얼마나 유사한지를 판단하는 스코어를 말한다. 

- 닷-프로덕트 어텐션에서는 이 스코어 값을 구하기 위해 s를 전치하고 각 은닉상태와 내적을 수행한다. 

- 현재 시점의 은닉상태의 어텐션 스코어를 구하고, 여기에 소프트맥스 함수를 적용하면 합이 1이 되는 확률분포를 얻게 되는데 이를 **어텐션 분포**라고 한다.

- 어텐션의 최종 결과값을 얻기 위해서는 각 인코더의 은닉상태와 어텐션 가중치를 곱하고 최종적으로 다 더한다. 

- 어텐션 값을 구하면 이를 디코더의 현재시점 s와 결합해 하나의 벡터로 만드는 작업을 수행한다. 이를 v라고 정의한다. v를 이용하여 최종 출력층의 입력으로 사용한다.



출처 

[1) 어텐션 메커니즘 (Attention Mechanism) - 딥 러닝을 이용한 자연어 처리 입문](https://wikidocs.net/22893)

[2)Sooftware : 네이버 블로그](https://blog.naver.com/sooftware/221784419691)


